#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import argparse
import pickle as pkl
from collections import OrderedDict

import numpy as np

def create_dict(sentences, min_freq, output_file):
    word_freqs = OrderedDict()
    l_sentences = len(sentences)
    for idx, sent in enumerate(sentences):
        # Collect frequencies
        for w in sent.split(' '):
            if w not in word_freqs:
                word_freqs[w] = 0
            word_freqs[w] += 1

        if (idx+1) % 10000 == 0:
            print('\r%d/%d processed' % (idx + 1, l_sentences), end=' ')
            sys.stdout.flush()

    print()

    # Remove already available <eos> and <unk> if any
    if '<eos>' in word_freqs:
        del word_freqs['<eos>']
    if '<unk>' in word_freqs:
        del word_freqs['<unk>']

    words = list(word_freqs.keys())
    freqs = np.array(list(word_freqs.values()))

    # Some heuristic to warn against non-tokenized data
    if "." not in words or "," not in words:
        print("(You can ignore this if the input doesn't contain punctuations.)")
        print("WARNING: Check that the input data is tokenized!")

    # Sort in descending order of frequency
    sorted_idx = np.argsort(freqs)
    sorted_words = [words[ii] for ii in \
                    sorted_idx[::-1] if freqs[ii] >= min_freq]

    print("# of unique words in %s: %d" % (filename, len(sorted_words)))

    worddict = OrderedDict()
    worddict['<eos>'] = 0
    worddict['<unk>'] = 1

    # Start inserting from index 2
    for ii, ww in enumerate(sorted_words):
        worddict[ww] = ii + 2

    print("Dumping vocabulary (%d tokens) to %s..." % (len(worddict), output_file))
    with open(output_file, 'wb') as f:
        pkl.dump(worddict, f)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog='build_dictionary')
    parser.add_argument('-o', '--output-dir', type=str, default='.', help='Output directory')
    parser.add_argument('-m', '--min-freq', type=int, default=0, help='Filter out words occuring < m times.')
    parser.add_argument('files', type=str, nargs='+', help='Text files to create dictionaries or sqlite database.')
    args = parser.parse_args()

    for filename in args.files:
        filename = os.path.abspath(os.path.expanduser(filename))
        vocab_fname = os.path.basename(filename)
        if args.min_freq > 0:
            vocab_fname += "-min%d" % args.min_freq
        vocab_fname = os.path.join(args.output_dir, vocab_fname)

        sentences = []
        print("Reading file %s" % filename)
        with open(filename) as f:
            for line in f:
                line = line.strip()
                if line:
                    sentences.append(line)

        create_dict(sentences, args.min_freq, vocab_fname + '.vocab.pkl')
