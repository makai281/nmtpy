#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Translates a source file using a translation model."""

import os
import sys
import time
import json
import atexit
import inspect
import argparse
import importlib
from multiprocessing import Process, Queue, cpu_count

from collections import OrderedDict

import numpy as np

from nmtpy.logger           import Logger
from nmtpy.config           import Config
from nmtpy.metrics          import get_scorer
from nmtpy.nmtutils         import idx_to_sent
from nmtpy.textutils        import reduce_to_best
from nmtpy.sysutils         import *
from nmtpy.filters          import get_filter
from nmtpy.iterators.bitext import BiTextIterator
from nmtpy.defaults         import INT, FLOAT

import nmtpy.cleanup as cleanup

# Setup the logger
Logger.setup()
log = Logger.get()

"""Worker process which does beam search."""
def translate_model(rqueue, wqueue, pid, model, beam_size, nbest,
                    suppress_unks, get_att_alphas=False, seed=1234, mode="beamsearch"):

    # Build the sampler of the model
    model.build_sampler()

    # Get function call string
    if mode == "beamsearch":
        func_call = 'model.beam_search(data_dict.values(), beam_size=beam_size, get_att_alphas=%s, suppress_unks=%s)' % (get_att_alphas, suppress_unks)

    elif mode in ["forced", "sample"]:
        func_call = 'model.gen_sample(data_dict)'

    elif mode == "argmax":
        func_call = 'model.gen_sample(data_dict, argmax=True)'

    while True:
        # Get a sample
        req = rqueue.get()

        # NOTE: We should avoid this
        if req is None:
            break

        # Unpack sample idx and data_dict
        sample_idx, data_dict = req[0], req[1]

        # Get the translation, its score and alignments
        trans, score, align = eval(func_call)

        # normalize scores according to sequence lengths
        score = score / np.array([len(s) for s in trans])

        # Sort the scores and take the best(s) idx(s)
        best_idxs = np.argsort(score)[:nbest]
        trans = np.array(trans)[best_idxs]

        # Check for attention weights
        if align is not None:
            align = np.array(align)[best_idxs]

        # Send response back
        wqueue.put((sample_idx, trans, score[best_idxs], align))

class Translator(object):
    """Starts worker processes and waits for the results."""
    def __init__(self, args):
        self.beam_size = args.beam_size

        # Always lists provided by argparse (nargs:'+')
        self.src_files = args.src_files
        self.ref_files = args.ref_files

        # Collect processed source sentences in here
        # for further exporting to json
        self.export = args.export

        # Assume for now that a request for JSON exporting
        # assumes fetching attentional alphas as well.
        self.get_att_alphas = self.export

        # Fetch other arguments
        self.utf8           = False
        self.first          = args.first
        self.nbest          = args.nbest
        self.seed           = args.seed
        self.mode           = args.decoder
        self.n_jobs         = args.n_jobs
        self.valid_mode     = args.validmode
        self.model_file     = args.model
        self.suppress_unks  = args.suppress_unks

        # Post-processing filters
        self.filters = []

        # Create worker process pool
        self.processes = [None] * self.n_jobs

    def set_model_options(self):
        model_options = dict(np.load(self.model_file)['opts'].tolist())

        # Import the module
        self.__class = importlib.import_module("nmtpy.models.%s" % model_options['model_type']).Model

        # Create the model
        self.model = self.__class(seed=self.seed, logger=None, **model_options)
        self.model.load(self.model_file)
        self.model.set_dropout(False)

        # Check for post-processing filter
        if "filter" in model_options:
            log.info("Hypotheses and references will be processed by '%s' filter" % model_options['filter'])
            self.filters.append(get_filter(model_options['filter']))

        # Get inverted dictionary from the model itself
        self.trg_idict = self.model.trg_idict

        # Be compatible for both utf-8 and normal dictionaries
        if isinstance(self.trg_idict[2], unicode):
            self.utf8 = True

        #######################################################
        # Forced decoding/NMT rescoring for given src/trg pairs
        #######################################################
        if self.mode == "forced" and self.src_files and self.ref_files:
            log.info("Using only %s as reference file for forced decoding." % self.ref_files[0])
            self.iterator = BiTextIterator(
                                        batch_size=1,
                                        srcfile=self.src_files[0], srcdict=self.model.src_dict,
                                        trgfile=self.ref_files[0], trgdict=self.model.trg_dict,
                                        n_words_src=self.model.n_words_src,
                                        n_words_trg=self.model.n_words_trg,
                                        trg_name='y_true')
            self.iterator.read()

        #########################
        # Normal translation mode
        #########################
        else:
            if self.src_files is not None:
                # Pass the files to the model
                # NOTE: Not quite model agnostic way of doing things.
                self.model.data['valid_src'] = self.src_files[0]
                if 'valid_img' in self.model.data:
                    self.model.data['valid_img'] = self.src_files[1]

                if self.ref_files is not None:
                    self.model.data['valid_trg'] = self.ref_files

            # Initialize model's validation data iterator
            # NOTE: data_mode is for best-source-selection decoding for multimodal systems
            if 'data_mode' in inspect.getargspec(self.model.load_valid_data).args:
                self.model.load_valid_data(from_translate=True, data_mode=self.valid_mode)
            else:
                self.model.load_valid_data(from_translate=True)

            # Set self.iterator to self.model.valid_iterator
            self.iterator = self.model.valid_iterator

            # Full or partial decoding given by -f argument
            if self.first > 0:
                # Only first self.first sentences
                self.n_sentences = self.first
            else:
                # All sentences
                self.n_sentences = self.iterator.n_samples

            log.info('I will translate %d samples' % self.n_sentences)

            # Assume validation data encoded in the model
            if self.src_files is None:
                log.info("No test data given, assuming validation dataset.")

                self.src_files = listify(self.model.data['valid_src'])

                # User may provide another reference set in 'valid_trg_orig' for example
                # with compound splitting reverted so that we can compute
                # the metrics correctly.
                # NOTE: May be avoided by using filters on reference sentences.
                if "valid_trg_orig" in self.model.data:
                    self.ref_files = listify(self.model.data['valid_trg_orig'])
                else:
                    self.ref_files = listify(self.model.valid_ref_files)

        # Print information
        log.info("Source file(s)")
        for f in self.src_files:
            log.info("  %s" % f)

        if self.ref_files:
            log.info("Reference file(s)")
            for f in self.ref_files:
                log.info("  %s" % f)

    def start(self):
        # create input and output queues for processes
        write_queue = Queue()
        read_queue  = Queue()

        # Create processes
        for idx in xrange(self.n_jobs):
            self.processes[idx] = Process(target=translate_model,
                                          args=(write_queue, read_queue, idx, self.model, self.beam_size,
                                          self.nbest, self.suppress_unks, self.get_att_alphas,
                                          self.seed, self.mode))
            # Start process and register for cleanup
            self.processes[idx].start()
            cleanup.register_proc(self.processes[idx].pid)

        cleanup.register_handler()

        # Send data to worker processes
        for idx in xrange(self.n_sentences):
            write_queue.put((idx, next(self.iterator)))

        log.info("Distributed %d sentences to worker processes." % self.n_sentences)

        # Receive the results
        self.trans       = [None] * self.n_sentences
        self.scores      = [None] * self.n_sentences

        # Will be filled if --export is passed
        self.att_weights = [None] * self.n_sentences

        # Performance computation stuff
        start_time = per100_time = time.time()

        for i in xrange(self.n_sentences):
            # Get response from worker
            resp = read_queue.get()

            # This is the sample id of the processed sample
            sample_idx = resp[0]

            # Get the hypotheses, scores and attention weights if any
            hyps, self.scores[sample_idx], attw = resp[1:]

            # Did we receive attention weights from beam search?
            if attw is not None:
                self.att_weights[sample_idx] = attw[0]

            outs = []

            for hyp in hyps:
                hyp = idx_to_sent(self.trg_idict, hyp)

                # Apply post-processing filters like compound stitching
                for filt in self.filters:
                    hyp = filt(hyp)

               # Append the actual hypothesis
                outs.append(hyp)

            # Place the hypotheses into their relevant places
            self.trans[sample_idx] = outs

            # Print progress
            if (i+1) % 100 == 0:
                per100_time = time.time() - per100_time
                log.info("%4d/%d sentences completed (%.2f seconds)" % ((i+1), self.n_sentences, per100_time))
                per100_time = time.time()

        # Total time spent during beam search
        total_time = time.time() - start_time
        t_per_sent = total_time / self.n_sentences

        log.info("-------------------------------------------")
        log.info("Total decoding time: %3.3f seconds (~%3.3f seconds / sentence)" % (total_time, t_per_sent))

        # Compute word-based time statistics as well
        if self.nbest == 1:
            n_words = float(sum([len(s[0].split(' ')) for s in self.trans]))
            t_per_word = total_time / n_words
            log.info("~%d words / second" % int((60. / t_per_word)))

        # Stop workers
        for pidx in xrange(self.n_jobs):
            write_queue.put(None)
            self.processes[pidx].terminate()
            cleanup.unregister_proc(self.processes[pidx].pid)

    def write_hyps(self, filename, dump_scores=False):
        def __encode(s):
            return s.encode('utf-8') if self.utf8 else s

        # Write file
        with open(filename, 'w') as f:
            if self.mode == "forced" or dump_scores:
                # We have a single hypothesis and a score for each sentence
                for idx, (tr, sc) in enumerate(zip(self.trans, self.scores)):
                    f.write(__encode("%d ||| %s ||| %.6f\n" % (idx, tr[0], sc)))

            elif self.nbest > 1:
                # We have n hypotheses and n scores for each sentence
                for idx, (trs, scs) in enumerate(zip(self.trans, self.scores)):
                    for tr, sc in zip(trs, scs):
                        f.write(__encode("%d ||| %s ||| %.6f\n" % (idx, tr, sc)))
            else:
                if self.valid_mode == 'pairs':
                    # Pick the best hyp out of all source sentences for a single image
                    self.trans = reduce_to_best(self.trans, self.scores,
                                                self.iterator.n_unique_images, avoid_unk=True)
                # Prepare and dump
                self.hyps = [s[0] for s in self.trans]
                hyps = "\n".join(self.hyps) + "\n"
                f.write(__encode(hyps))

    def compute_metrics(self, hyp_file, scorers):
        """Computes evaluation metrics for the hypotheses."""
        results = {}
        for scorer in scorers:
            c = get_scorer(scorer)()
            score = c.compute(self.ref_files, hyp_file)
            results[scorer] = (str(score), score.score)
        self.results = results
        return results

    def dump_json(self, filename):
        """Export decoding data into json for further visualization."""
        metadata = OrderedDict()
        metadata['model']     = os.path.basename(self.model_file)
        metadata['beam_size'] = self.beam_size

        srcs    = []
        refs    = []
        samples = []

        # Reset iterator
        self.iterator.rewind()
        for i in range(self.n_sentences):
            data = next(self.iterator)
            if 'x' in data:
                srcs.append(idx_to_sent(self.model.src_idict, data['x'].flatten()))

        # Save metadata
        data = {'metadata' : metadata}

        # Open reference files
        all_refs = [open(f).read().strip().split("\n") for f in self.ref_files]
        n_refs = len(all_refs)
        n_samples = len(all_refs[0])

        mult_source = False
        if len(all_refs[0]) != len(srcs):
            # Multiple sources given
            mult_source = True

        # Collect reference sentences
        for sidx in range(self.n_sentences):
            sidx = sidx if not mult_source else sidx % len(all_refs[0])
            refs.append([all_refs[i][sidx] for i in range(n_refs)])

        # Add sources, targets, and references
        for s, t, att in zip(srcs, self.trans, self.att_weights):
            sample = {'src' : s.split(' '), 'trg' : t[0].split(' '), 'ref' : refs.pop(0), 'att': att}
            samples.append(sample)
        data['data'] = samples

        # Export the JSON
        def _default(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()

        with open(filename, 'w') as f:
            json.dump(data, f, default=_default)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog='nmt-translate')
    parser.add_argument('-f', '--first'         , type=int, default=0,      help="How many sentences should be translated, useful for debugging.")
    parser.add_argument('-j', '--n-jobs'        , type=int, default=8,      help="Number of processes (default: 8, 0: Auto)")
    parser.add_argument('-b', '--beam-size'     , type=int, default=12,     help="Beam size (only for beam-search)")
    parser.add_argument('-N', '--nbest'         , type=int, default=1,      help="N for N-best output (only for beam-search)")
    parser.add_argument('-r', '--seed'          , type=int, default=1234,   help="Random number seed for sampling mode (default: 1234)")

    parser.add_argument('-v', '--validmode'     , default='single',         help="Validation mode for WMT16 MMT Task2: all/pairs/single")
    parser.add_argument('-D', '--decoder'       , default='beamsearch',     choices=['beamsearch', 'argmax', 'sample', 'forced'], help="Decoding mode")

    parser.add_argument('-m', '--model'         , type=str, required=True,  help="Model file")
    parser.add_argument('-M', '--metrics'       , type=str, default='bleu', help="Comma separated list of metrics (bleu or bleu,meteor)")
    parser.add_argument('-o', '--saveto'        , type=str, default=None,   help="Output translations file (if not given, only metrics will be printed)")
    parser.add_argument('-e', '--export'        , action='store_true',      help="Export all decoding process to json for visualization")
    parser.add_argument('-s', '--score'         , action='store_true',      help="Print scores of each sentence even nbest == 1")
    parser.add_argument('-u', '--suppress-unks' , action='store_true',      help="Don't produce <unk>'s in beam search")

    parser.add_argument('-S', '--src-files'     , type=str, nargs='+', default=None, help="Source data(s) in order: text,image (default: validation set)")
    parser.add_argument('-R', '--ref-files'     , type=str, nargs='+', default=None, help="One or multiple reference files (default: validation set)")

    args = parser.parse_args()

    if args.decoder == "forced" and (args.src_files is None or args.ref_files is None):
        print "Error: Forced decoding requires that you give src and ref files explicitly."
        sys.exit(1)

    if args.n_jobs == 0:
        # Auto infer CPU number
        args.n_jobs = (cpu_count() / 2) - 1

    if args.n_jobs > 1:
        # This is to avoid thread explosion. Allow
        # each process to use a single thread.
        os.environ["OMP_NUM_THREADS"] = "1"
        os.environ["MKL_NUM_THREADS"] = "1"

    # Force CPU
    os.environ["THEANO_FLAGS"] = "device=cpu"

    # Print some informations
    log.info("%d CPU processes - beam size = %2d" % (args.n_jobs, args.beam_size))

    # Create translator object
    translator = Translator(args)
    translator.set_model_options()
    translator.start()

    out_file = args.saveto
    if not args.saveto:
        # Override this if given
        args.score = False
        hypf = get_temp_file(suffix=".nbest_hyps")
        out_file = hypf.name
        hypf.close()

    # Dump hypotheses
    translator.write_hyps(out_file, args.score)

    if args.export and args.saveto:
        # Export attentional informations if -o and -e are given
        translator.dump_json("%s.json" % out_file)

    # No need to compute metrics with nbest style files
    if args.decoder != "forced" and args.nbest == 1 \
            and translator.ref_files and not args.score:
        # Compute all metrics
        results = translator.compute_metrics(out_file, args.metrics.split(","))
        # NOTE: This dict is expected from nmt-translate for obtaining the validation results.
        print results

    sys.exit(0)
