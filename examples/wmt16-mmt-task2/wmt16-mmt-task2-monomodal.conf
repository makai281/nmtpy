[training]
# This is the attention NMT with WMT iterator
model_type: attention_wmt
patience: 10
max_epochs: 100
valid_freq: 0
valid_metric: meteor
decay_c: 1e-5
clip_c: 5
seed: 1234

[model]
tied_trg_emb: True
layer_norm: True
shuffle_mode: trglen
embedding_dim: 100
rnn_dim: 100

optimizer: adam
lrate: 0.0004
weight_init: xavier
batch_size: 32

n_words_src: 0

# Only the most 10K, other -> UNK
n_words_trg: 10000

save_path: ~/nmtpy/models

[model.dicts]
src: ~/nmtpy/data/wmt16-task2/train_src.pkl
trg: ~/nmtpy/data/wmt16-task2/train_trg.pkl

[model.data]
train_src: ~/nmtpy/data/wmt16-task2/flickr_30k_align.train.pkl
valid_src: ~/nmtpy/data/wmt16-task2/flickr_30k_align.valid.pkl
valid_trg: ~/nmtpy/data/wmt16-task2/valid.*.tok.lc.nopunct.de
